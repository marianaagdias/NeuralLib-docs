{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NeuralLib Documentation","text":""},{"location":"#neurallib-documentation","title":"NeuralLib Documentation","text":""},{"location":"#overview","title":"Overview","text":"<p><code>NeuralLib</code> is a Python library designed for advanced biosignal processing using neural networks. The core concept revolves around creating, training, and managing neural network models and leveraging their components for transfer learning (TL). This allows for the reusability of pre-trained models or parts of them to create new models and adapt them to different tasks or datasets efficiently.</p> <p>The library supports:</p> <ul> <li>Training and testing <code>Architectures</code> from scratch for specific biosignals processing tasks.</li> <li>Adding tested models to hugging face repositories to create <code>ProductionModels</code> and share them with the community for public usage.</li> <li>Extracting trained components from production models using <code>TLFactory</code>.</li> <li>Combining, freezing, or further fine-tuning pre-trained components to train<code>TLModels</code>.</li> </ul>"},{"location":"#end-users","title":"End users","text":"<p>NeuralLib is designed to support two types of users:</p> <ul> <li> <p>General Users (pip package users)</p> <p>These users do not need to train models from scratch. Instead, they can install <code>NeuralLib</code> via <code>pip</code> and use pre-trained models available in the <code>model_hub</code> module. This allows them to apply biosignal processing models from NeuralLib collection in hugging face (Collection) without requiring deep knowledge of neural networks.</p> <p>Install with:</p> <pre><code>pip install NeuralLib\n</code></pre> <p>Use pre-trained models with minimal setup:</p> <pre><code>from NeuralLib.model_hub import ProductionModel\nmodel = ProductionModel.load_from_huggingface(\"model-name\")\npredictions = model.predict(input_signal)\n</code></pre> </li> <li> <p>Developers &amp; Researchers (GitHub users)</p> <p>These users need full access to the library's source code for model development, training, and fine-tuning. They can:</p> <ul> <li>Train new models from scratch using the <code>architectures</code> module</li> <li>Test and validate models before sharing them with the community</li> <li>Extend <code>NeuralLib</code> with custom architectures and transfer learning workflows</li> </ul> <p>Clone the repository for development:</p> <pre><code>git clone https://github.com/marianaagdias/NeuralLib.git\ncd NeuralLib\npip install -e .\n</code></pre> <p>Train a model from scratch: (for more detailed examples, check the Tutorials folder)</p> <pre><code>from NeuralLib.architectures import GRUseq2seq\nmodel = GRUseq2seq(model_name=\"test\", n_features=10, hid_dim=64, n_layers=2, dropout=0.1, learning_rate=0.001)\nmodel.train_from_scratch(path_x=\"data/train_x.npy\", path_y=\"data/train_y.npy\", batch_size=32, epochs)\n</code></pre> </li> </ul>"},{"location":"#functionalities","title":"Functionalities","text":"<p>Two main modules:</p> <ol> <li> <p>architectures</p> </li> <li> <p>model_hub</p> </li> </ol> <p>Utils and config: utils config</p>"},{"location":"architectures/","title":"architectures","text":""},{"location":"architectures/#overview","title":"Overview","text":"<p>The <code>architectures</code> subpackage provides a framework for defining, training, and retraining various neural network architectures tailored for biosignal processing. The package supports:</p> <p>\u2714 Training from scratch and retraining with configurable hyperparameters.</p> <p>\u2714Model checkpointing &amp; hyperparameter optimization via grid search with PyTorch Lightning.</p> <p>\u2714Integration with TensorBoard for visualization of training metrics.</p>"},{"location":"architectures/#package-structure","title":"Package Structure","text":"<ol> <li>Architecture Class and related functions<ul> <li>Definition of <code>Architecture</code> class, which is extended by all neural network models in the package.</li> <li>Utility functions for checkpoint management, hyperparameter extraction, training and retraining, and testing.</li> </ul> </li> <li>Biosignals Architectures<ul> <li>Implements neural network architectures designed for biosignal processing tasks, including GRU-based models (e.g., <code>GRUseq2seq</code>, <code>GRUEncoderDecoder</code>) and Transformer-based models (<code>Transformerseq2seq</code>). These architectures are optimized for sequence modeling tasks such as signal denoising or timeseries classification.</li> </ul> </li> <li>Training and Deployment Workflow in NeuralLib<ul> <li>Provides high-level functions for training and retraining models dynamically based on user-defined parameters.</li> </ul> </li> <li><code>upload_to_hugging</code> : enables uploading trained models to Hugging Face for production.</li> <li><code>post_process_fn</code>: contains post-processing utilities for handling predictions.</li> </ol>"},{"location":"architectures/#hands-on","title":"Hands on","text":"<p>Check Tutorials:</p> <ol> <li> <p>Tutorial #1: Train, retrain, and test a biosignals model.</p> <p>Key methods: <code>train_from_scratch()</code>, <code>retrain()</code> and <code>test_on_test_set()</code></p> </li> <li> <p>Tutorial #2: Hyperparameter optimization using grid search. Key methods: <code>run_grid_search</code>, <code>get_hparams_from_checkpoints</code>, <code>test_on_single_signal</code></p> </li> <li>Tutorial #3: Convert a trained model into a production-ready model. Key method: <code>upload_production_model()</code></li> </ol>"},{"location":"architectures/#best-practices","title":"Best Practices","text":"<ol> <li>Directory Management:<ul> <li>Ensure checkpoint directories are accessible and writable.</li> </ul> </li> <li>Dataset formatting:<ul> <li>Ensure datasets are structured correctly: input (path_x) and output (path_y) each with \u2018train\u2019, \u2018test\u2019, and \u2018val\u2019 subfolders.</li> <li>(optional) Add the dataset directory to config.file</li> <li>biosignal datasets should be in .npy files: each signal in a separate file.</li> </ul> </li> <li>Logging:<ul> <li>Enable TensorBoard logging for detailed monitoring of training progress.</li> </ul> </li> </ol>"},{"location":"base/","title":"Architecture Class","text":""},{"location":"base/#class-architecture","title":"Class: <code>Architecture</code>","text":"<p><code>Architecture</code> class provides a framework for training, retraining, testing, and managing neural network models tailored for biosignal processing. It integrates with PyTorch Lightning to streamline training workflows, support checkpoint management, and enable logging via TensorBoard. It also includes utilities for loading pre-trained models from local directories or Hugging Face.</p>"},{"location":"base/#initialization","title":"Initialization","text":""},{"location":"base/#__init__self-architecture_name","title":"<code>__init__(self, architecture_name)</code>","text":"<ul> <li>Defines a base neural network model with <code>architecture_name</code>.</li> <li>Initializes training history and checkpoint directory.</li> </ul>"},{"location":"base/#checkpoint-and-training-information-management","title":"Checkpoint and Training Information Management","text":""},{"location":"base/#create_checkpoints_directoryself-retraining","title":"<code>create_checkpoints_directory(self, retraining)</code>","text":"<ul> <li>Creates a unique directory for storing model checkpoints.</li> <li>Uses architecture parameters and timestamp for directory naming.</li> <li>The directory follows the format: <code>{RESULTS_BASE_DIR}/{model_name}/checkpoints/{architecture_name}_{hid_dim}hid_{n_layers}l_lr{learning_rate}_drop{dropout}_{timestamp}</code>.</li> </ul> <p>If retraining, the directory name includes <code>_retraining</code> to differentiate from the initial training run.</p>"},{"location":"base/#save_training_informationself-trainer-optimizer-train_dataset_name-trained_for-val_loss-total_training_time-gpu_model-retraining-prev_training_historynone","title":"<code>save_training_information(self, trainer, optimizer, train_dataset_name, trained_for, val_loss, total_training_time, gpu_model, retraining, prev_training_history=None)</code>","text":"<ul> <li>Stores information about the current training process, including dataset, optimizer, validation loss, and training time.</li> <li>If retraining, appends previous training history.</li> </ul>"},{"location":"base/#save_training_information_to_fileself-directory","title":"<code>save_training_information_to_file(self, directory)</code>","text":"<ul> <li>Saves the training information dictionary to <code>training_info.json</code> in the given directory.</li> </ul>"},{"location":"base/#model-training-and-retraining","title":"Model Training and Retraining","text":""},{"location":"base/#train_from_scratchself-path_x-path_y-patience-batch_size-epochs-gpu_idnone-all_samplesfalse-samplesnone-dataset_namenone-trained_fornone-classificationfalse-enable_tensorboardfalse","title":"<code>train_from_scratch(self, path_x, path_y, patience, batch_size, epochs, gpu_id=None, all_samples=False, samples=None, dataset_name=None, trained_for=None, classification=False, enable_tensorboard=False)</code>","text":"<ul> <li>Trains a model from scratch using PyTorch Lightning.</li> <li>Initializes model, dataset, dataloaders, and training parameters.</li> <li>Saves best checkpoint and logs training progress using TensorBoard.</li> <li>Saves final weights in <code>.pth</code> format.</li> </ul> <p>Detailed breakdown of the function:</p> <ol> <li>Setup &amp; Configuration:</li> </ol> <p>The random seed is set, the device (CPU/GPU) is configured using <code>configure_device(gpu_id)</code>. Checks if TensorBoard is available.</p> <ol> <li>Model Initialization &amp; Checkpoints:</li> </ol> <p>A directory for storing model checkpoints is created in  <code>&lt;DEV_BASE_DIR&gt;/results/&lt;model_name&gt;/checkpoints/&lt;architecture_name_hparams_datetime&gt;</code></p> <ol> <li>Dataset &amp; DataLoader Preparation:</li> </ol> <p>Training and validation datasets are instantiated from <code>DatasetSequence</code>, loading data from <code>path_x</code> (features) and <code>path_y</code> (labels). PyTorch <code>DataLoader</code> objects are created. The <code>collate_fn</code> function is used to handle dynamic sequence lengths.</p> <ol> <li>Defining Callbacks for Training:<ul> <li>Checkpoint Callback: Saves the best model based on validation loss (<code>val_loss</code>).</li> <li>Early Stopping Callback: Stops training early if validation loss doesn't improve for <code>patience</code> epochs.</li> <li>Loss Plot Callback: Saves a loss curve to visualize training progress.</li> </ul> </li> <li>Trainer Initialization &amp; Logging:<ul> <li>If TensorBoard is enabled, a <code>TensorBoardLogger</code> is set up for tracking metrics and hyperparameters (hparams.yaml)</li> <li>The PyTorch Lightning <code>Trainer</code> is instantiated, specifying: maximum epochs (<code>epochs</code>), device, callbacks (Checkpointing, Early Stopping, Loss Plot), logging</li> </ul> </li> <li>Training Execution:</li> </ol> <p>The model is trained using <code>trainer.fit(model, train_dataloader, val_dataloader)</code>.</p> <ol> <li>Post-Training Processing &amp; Model Saving:<ul> <li>The best (lowest) validation loss is extracted from the checkpoint callback.</li> <li>Training metadata (trainer state, optimizer, dataset, GPU info, loss, etc.) is saved (using <code>model.save_training_information()</code>) and written to <code>training_info.json</code> inside the checkpoint directory.</li> <li>The final model weights (corresponding to the lowest validation loss) are saved in <code>model_weights.pth</code> inside the checkpoint directory.</li> </ul> </li> </ol>"},{"location":"base/#retrainself-path_x-path_y-patience-batch_size-epochs-gpu_idnone-all_samplesfalse-samplesnone-dataset_namenone-trained_fornone-classificationfalse-enable_tensorboardfalse-checkpoints_directorynone-hugging_face_modelnone","title":"<code>retrain(self, path_x, path_y, patience, batch_size, epochs, gpu_id=None, all_samples=False, samples=None, dataset_name=None, trained_for=None, classification=False, enable_tensorboard=False, checkpoints_directory=None, hugging_face_model=None)</code>","text":"<ul> <li>Loads model weights from a previous checkpoint or Hugging Face repository.</li> <li>Performs training while preserving previous history. The main difference to <code>train_from_scratch</code> is<ul> <li>retraining of the whole model (can be either to use additional/different data, to train for more epochs,..). If the purpose is to perform TL, use the <code>TLModel</code> class from <code>model_hub</code> package.</li> </ul> </li> <li>Saves updated training information and final model weights.</li> </ul>"},{"location":"base/#model-testing-and-evaluation","title":"Model Testing and Evaluation","text":""},{"location":"base/#test_on_test_setself-path_x-path_y-all_samplestrue-samplesnone-checkpoints_dirnone-gpu_idnone-save_predictionsfalse-post_process_fnnone","title":"<code>test_on_test_set(self, path_x, path_y, all_samples=True, samples=None, checkpoints_dir=None, gpu_id=None, save_predictions=False, post_process_fn=None)</code>","text":"<ul> <li>Evaluates the trained model on a test dataset.</li> <li>Loads model weights from the checkpoint directory.</li> <li>Returns predictions and average test loss.</li> <li>Optionally saves predictions and applies post-processing functions.</li> </ul>"},{"location":"base/#test_on_single_signalself-x-checkpoints_dirnone-gpu_idnone-post_process_fnnone","title":"<code>test_on_single_signal(self, X, checkpoints_dir=None, gpu_id=None, post_process_fn=None)</code>","text":"<ul> <li>Tests the model on a single input signal.</li> <li>Loads model weights from the checkpoint directory.</li> <li>Returns the predicted output, optionally applying post-processing.</li> </ul>"},{"location":"base/#auxiliary-functions-and-methods","title":"(Auxiliary) Functions and Methods","text":""},{"location":"base/#1-get_weights_and_info_from_checkpointsprev_checkpoints_dir","title":"1. <code>get_weights_and_info_from_checkpoints(prev_checkpoints_dir)</code>","text":"<ul> <li>Extracts model weights (<code>.pth</code>) and training history (<code>training_info.json</code>) from a given checkpoint directory.</li> <li>If <code>.pth</code> is unavailable, converts <code>.ckpt</code> to <code>.pth</code>.</li> <li>Returns the path to the weights file and training history.</li> </ul>"},{"location":"base/#2-get_weights_and_info_from_hugginghugging_face_model-local_dirnone","title":"2. <code>get_weights_and_info_from_hugging(hugging_face_model, local_dir=None)</code>","text":"<ul> <li>Downloads model weights and training history from a Hugging Face repository.</li> <li>Returns the local path to the weights and training history file.</li> </ul>"},{"location":"base/#3-get_hparams_from_checkpointscheckpoints_directory","title":"3. <code>get_hparams_from_checkpoints(checkpoints_directory)</code>","text":"<ul> <li>Retrieves hyperparameters from the latest version of <code>hparams.yaml</code> in a checkpoint directory.</li> <li>Ensures the existence of relevant log directories.</li> </ul> <p>Example (from tutorial 2 - grid search):</p> <pre><code>from NeuralLib.architectures import get_hparams_from_checkpoints, GRUseq2seq\narchitecture_params = get_hparams_from_checkpoints(best_dir)\n# Initialize the model using the loaded parameters\nmodel = GRUseq2seq(**architecture_params)\n</code></pre>"},{"location":"base/#4-get_hparams_from_hugginghugging_face_model","title":"4. <code>get_hparams_from_hugging(hugging_face_model)</code>","text":"<ul> <li>Downloads hyperparameters (<code>hparams.yaml</code>) from a Hugging Face repository.</li> <li>Returns hyperparameters as a dictionary.</li> </ul>"},{"location":"base/#5-validate_training_contextretraining-checkpoints_directory-hugging_face_model","title":"5. <code>validate_training_context(retraining, checkpoints_directory, hugging_face_model)</code>","text":"<ul> <li>Ensures correct conditions for training (<code>retraing=False</code>) and retraining (<code>retraing=True</code>).</li> <li>Raises errors if checkpoints are missing when retraining or provided when training from scratch.</li> </ul>"},{"location":"biosignals_architectures/","title":"Biosignals Architectures","text":""},{"location":"biosignals_architectures/#key-components","title":"Key Components","text":"<p>Specialized neural network architectures tailored for biosignal processing. Each model inherits from the base <code>Architecture</code> class, allowing consistent training workflows, checkpoint handling, and retraining capabilities.</p>"},{"location":"biosignals_architectures/#general-overview","title":"General Overview","text":"<p>Biosignals processing encompasses a variety of tasks, each requiring different approaches depending on the nature of the input signals and the desired outputs. The choice of architecture and structure is dictated by the specific task, such as regression, classification, or signal generation.</p> <p>The tasks can be grouped as follows:</p> <ol> <li> <p>Signal Transformation (Sequence-to-Sequence)</p> <p>Tasks where the output is a transformed signal with the same temporal structure as the input.</p> <p>Examples:</p> <ul> <li>Regression: Predicting continuous signal values (e.g., filtering noise from signals).</li> <li>Classification: Assigning labels to each time step in the input sequence (e.g., activity classification in biosignals).</li> </ul> </li> <li> <p>Signal Generation (Encoder-Decoder)</p> <p>Tasks where the model generates an output signal from a compressed representation of the input.</p> <p>Examples:</p> <ul> <li>Synthesizing signals.</li> <li>Reconstructing signals from compressed or incomplete data.</li> </ul> </li> <li> <p>Information Extraction (Sequence-to-One)</p> <p>Tasks where the goal is to extract high-level information or a summary statistic from the signal.</p> <p>Examples:</p> <ul> <li>Regression: Predicting a continuous value (e.g., mean heart rate over a signal segment).</li> <li>Classification: Determining a single label for the entire input sequence (e.g., detecting arrhythmia).</li> </ul> </li> </ol>"},{"location":"biosignals_architectures/#1-seq2one-true-predicts-a-single-output-for-the-entire-sequence","title":"1. Seq2one = True (Predicts a single output for the entire sequence)","text":"Task Loss Function Correct <code>y</code> Shape 1.1. Classification 1.1.1 Multi-label (including binary, <code>num_classes=1</code>) BCEWithLogitsLoss <code>[1, num_classes]</code> (batch size will be added automatically in DataLoader) 1.1.2 Multiclass CrossEntropyLoss <code>() (scalar)</code> (Must be a single class index, not one-hot encoded) 1.2 Regression MSELoss <code>[1, num_features]</code> (if predicting multiple values, otherwise <code>[1, 1]</code> if a single scalar)"},{"location":"biosignals_architectures/#2-seq2seq-or-encoder-decoder","title":"2. Seq2seq or Encoder-Decoder","text":"Task Loss Function Correct <code>y</code> Shape 2.1. Classification 2.1.1 Multi-label (including binary, <code>num_classes=1</code>) BCEWithLogitsLoss <code>[sequence_length, num_classes]</code> (Each timestep has a multi-label prediction with <code>num_classes</code> probabilities) 2.1.2 Multiclass CrossEntropyLoss <code>[sequence_length]</code> (Each timestep gets a single class index, like <code>torch.LongTensor([1, 2, 0, 3])</code>) 2.2 Regression MSELoss <code>[sequence_length, num_features]</code> (Matches input shape, as it predicts a value at each timestep)"},{"location":"biosignals_architectures/#module-structure","title":"Module Structure","text":""},{"location":"biosignals_architectures/#classes","title":"Classes","text":""},{"location":"biosignals_architectures/#1-gruseq2seq","title":"1. <code>GRUseq2seq</code>","text":"<ul> <li>Implements a sequence-to-sequence GRU-based model.</li> <li>Supports variable-length input sequences using PyTorch's <code>pack_padded_sequence</code> and <code>pad_packed_sequence</code>.</li> <li>Includes dropout layers for regularization.</li> <li>Uses <code>BCEWithLogitsLoss</code> for binary/multilabel classification and <code>CrossEntropyLoss</code> for multi-class tasks.</li> </ul>"},{"location":"biosignals_architectures/#2-gruseq2one","title":"2. <code>GRUseq2one</code>","text":"<ul> <li>Implements a GRU-based model for sequence-to-one tasks.</li> <li>Uses the last time step's hidden state to make predictions.</li> <li>Supports classification and regression tasks.</li> <li>Uses a similar checkpoint directory structure as <code>GRUseq2seq</code>.</li> </ul>"},{"location":"biosignals_architectures/#3-gruencoderdecoder","title":"3. <code>GRUEncoderDecoder</code>","text":"<ul> <li>Implements an encoder-decoder model using GRUs.</li> <li>Encodes input sequences into a hidden representation before decoding into an output sequence.</li> <li>Supports packed sequences for variable-length inputs.</li> <li>Uses Mean Squared Error (MSE) loss for regression tasks.</li> </ul>"},{"location":"biosignals_architectures/#4-transformerseq2seq","title":"4. <code>TransformerSeq2Seq</code>","text":"<ul> <li>Implements a Transformer-based sequence-to-sequence model.</li> <li>Utilizes <code>TransformerEncoder</code> and <code>TransformerDecoder</code> layers.</li> <li>Uses <code>MSELoss</code> for sequence regression tasks.</li> </ul>"},{"location":"biosignals_architectures/#5-transformerseq2one","title":"5. <code>TransformerSeq2One</code>","text":"<ul> <li>Implements a Transformer encoder-only model for sequence-to-one tasks.</li> <li>Uses only the last hidden state for prediction.</li> <li>Supports both classification and regression.</li> </ul>"},{"location":"biosignals_architectures/#6-transformerencoderdecoder","title":"6. <code>TransformerEncoderDecoder</code>","text":"<ul> <li>Implements a full Transformer encoder-decoder architecture.</li> <li>Uses <code>TransformerEncoder</code> to process input sequences and <code>TransformerDecoder</code> to generate outputs.</li> <li>Suitable for time-series prediction and reconstruction tasks.</li> </ul>"},{"location":"biosignals_architectures/#architectural-choices","title":"Architectural Choices","text":""},{"location":"biosignals_architectures/#why-grus","title":"Why GRUs?","text":"<p>Gated Recurrent Units (GRUs) are well-suited for biosignal data because they:</p> <ul> <li>Efficiently capture temporal dependencies in sequential data.</li> <li>Use gating mechanisms to mitigate issues like vanishing gradients in long sequences.</li> <li>Are computationally lighter than other recurrent architectures like LSTMs, making them suitable for biosignals with high temporal resolution.</li> </ul>"},{"location":"biosignals_architectures/#why-transformers","title":"Why Transformers?","text":"<p>Transformers are ideal for tasks where:</p> <ul> <li>Long-range dependencies in the signal need to be captured effectively.</li> <li>Parallel processing (enabled by self-attention mechanisms) provides computational advantages over sequential models like GRUs.</li> </ul>"},{"location":"config/","title":"Configuration","text":""},{"location":"config/#development-configuration-configpy","title":"Development Configuration (config.py)","text":""},{"location":"config/#assumed-folder-structure","title":"Assumed Folder Structure","text":"<p>When using config.py, the project expects a specific directory structure relative to the root directory (dev): <pre><code>dev/\n\u2502\u2500\u2500 NeuralLib/              # Library source code (contains `config.py`)\n\u2502\u2500\u2500 data/                   # Directory where datasets are stored\n\u2502\u2500\u2500 results/                # Stores model training logs, evaluation results, and checkpoints\n\u2502\u2500\u2500 hugging_prodmodels/     # Stores Hugging Face models locally before uploading\n</code></pre></p>"},{"location":"config/#defined-paths","title":"Defined Paths","text":"<ul> <li><code>DATA_BASE_DIR</code>: Points to <code>dev/data/</code>, where datasets are stored.</li> <li><code>RESULTS_BASE_DIR</code>: Points to <code>dev/results/</code>, where model training outputs are saved.</li> <li><code>HUGGING_MODELS_BASE_DIR</code>: Points to <code>dev/hugging_prodmodels/</code>, where production models are stored before uploading.</li> </ul> <p>If these directories do not exist, they are automatically created at runtime.</p>"},{"location":"dataset_requirements/","title":"Dataset Organization and Requirements","text":"<p>To ensure compatibility with <code>DatasetSequence</code>, datasets should be preprocessed and saved in a structured format before being used for training or inference. Below are the requirements and best practices for organizing datasets.</p>"},{"location":"dataset_requirements/#1-general-requirements","title":"1. General Requirements","text":"<ul> <li>Preprocessing: Data should be saved in a preprocessed state, meaning all necessary filtering or transformations (except min-max normalization, which can optionally be performed when passing the signals to the model) should be done before loading into the dataset.</li> <li>File Format: Data should be stored as <code>.npy</code> files for efficient loading.</li> <li>Consistency: Each input sample (<code>X</code>) must have a corresponding output (<code>Y</code>) with the same filename to ensure proper mapping (read next section).</li> </ul>"},{"location":"dataset_requirements/#2-directory-structure-and-file-naming-correspondence","title":"2. Directory Structure and File Naming Correspondence","text":"<p>Datasets should follow a structured directory format with separate folders for training, validation, and testing data.</p> <pre><code>\u2502\u2500\u2500 x/                   # Input signals (\"path_x\")\n\u2502   \u251c\u2500\u2500 train/           # Training set\n\u2502   \u2502   \u251c\u2500\u2500 sample_001.npy\n\u2502   \u2502   \u251c\u2500\u2500 sample_002.npy\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 val/             # Validation set\n\u2502   \u2502   \u251c\u2500\u2500 sample_101.npy\n\u2502   \u2502   \u251c\u2500\u2500 sample_102.npy\n\u2502   \u251c\u2500\u2500 test/            # Test set\n\u2502       \u251c\u2500\u2500 sample_201.npy\n\u2502       \u251c\u2500\u2500 sample_202.npy\n\n\u2502\u2500\u2500 y/                   # Output (\"path_y\")\n\u2502   \u251c\u2500\u2500 train/           # Training set\n\u2502   \u2502   \u251c\u2500\u2500 sample_001.npy   # Must match the filenames in x/train/\n\u2502   \u2502   \u251c\u2500\u2500 sample_002.npy\n\u2502   \u251c\u2500\u2500 val/             # Validation \n\u2502   \u2502   \u251c\u2500\u2500 sample_101.npy\n\u2502   \u2502   \u251c\u2500\u2500 sample_102.npy\n\u2502   \u251c\u2500\u2500 test/            # Test \n\u2502       \u251c\u2500\u2500 sample_201.npy   # Must match filenames in x/test/\n\u2502       \u251c\u2500\u2500 sample_202.npy\n</code></pre> <ul> <li>Each file in <code>x/train/</code>, <code>x/val/</code>, and <code>x/test/</code> must have a corresponding file in <code>y/train/</code>, <code>y/val/</code>, and <code>y/test/</code>, respectively. For example:</li> </ul> <pre><code>x/train/sample_001.npy  \u2194  y/train/sample_001.npy\n</code></pre>"},{"location":"dataset_requirements/#3-data-formatting-requirements","title":"3. Data Formatting Requirements","text":""},{"location":"dataset_requirements/#input-data-x-format","title":"Input Data (X) Format","text":"<ul> <li>Each input sample (<code>item_x</code>) must have shape <code>[seq_len, num_features]</code>.</li> <li>The number of features must match <code>n_features</code> specified when initializing the model.</li> <li>Automatic Formatting by <code>DatasetSequence</code>:<ul> <li>If input is a 1D signal (<code>[seq_len]</code>), it is reshaped to <code>[seq_len, 1]</code>.</li> <li>If input is incorrectly stored as <code>[num_features, seq_len]</code>, it is transposed before returning.</li> </ul> </li> </ul>"},{"location":"dataset_requirements/#output-data-y-format","title":"Output Data (Y) Format","text":"<ul> <li> <p>For sequence-to-one tasks (e.g., classification, regression):</p> <ul> <li>Each <code>item_y</code> should be a single value or vector, with shape:<ul> <li>Multiclass classification (single-label): <code>[1]</code> (a single integer representing the class index).</li> <li>Multilabel classification: <code>[1, num_classes]</code> (a binary vector where each position indicates whether a class is active).</li> <li>Regression: <code>[1, 1]</code> (a single scalar value for continuous outputs).</li> </ul> </li> </ul> </li> <li> <p>For sequence-to-sequence tasks:</p> <ul> <li>Regression: Each <code>item_y</code> must have shape <code>[seq_len, num_features]</code>, matching <code>item_x</code>.</li> <li>Multiclass classification (single-label): Each <code>item_y</code> should have shape <code>[seq_len]</code>, where each timestep contains an integer representing the class.</li> <li>Multilabel classification: Each <code>item_y</code> should have shape <code>[seq_len, num_classes]</code>, where each timestep has a binary vector for active classes.</li> </ul> </li> <li> <p>Automatic Formatting by <code>DatasetSequence</code>:</p> <ul> <li>If output is a 1D signal, it is reshaped to <code>[seq_len, 1]</code> if <code>seq2seq</code>, or <code>[1, num_classes]</code> if <code>seq2one</code> for multilabel classification.</li> <li>If output is incorrectly stored as <code>[num_features, seq_len]</code>, it is transposed before returning (for <code>seq2seq</code>).</li> <li>If output is a single value, it is reshaped to <code>[1, 1]</code> (for <code>seq2one</code> regression).</li> </ul> </li> </ul>"},{"location":"dataset_requirements/#sequence-lengths","title":"Sequence lengths","text":"<p>Sequence lengths can differ from signal to signal, as the signals are automatically padded if that is the case.</p>"},{"location":"dataset_requirements/#pre-processing","title":"Pre-processing","text":"<p>It is assumed that the datasets have been pre-processed. However, it is possible to perform minmax normalization of the signals (each signal individually) if min_max_norm_sig is set to True.</p>"},{"location":"model_hub/","title":"Model Hub","text":""},{"location":"model_hub/#overview","title":"Overview","text":"<p>The <code>model_hub</code> module in NeuralLib provides functionalities for loading, managing, and fine-tuning pre-trained models for biosignal processing. It allows users to:</p> <p>\u2714 Load pre-trained models from Hugging Face for immediate use.</p> <p>\u2714 Perform inference on biosignal data with minimal setup.</p> <p>\u2714 Inject pre-trained model weights into new architectures for Transfer Learning (TL).</p> <p>\u2714 Freeze/unfreeze layers to optimize training.</p> <p>\u2714 Extend models with new data while preserving learned features.</p> <p>This module is essential for users who want to apply state-of-the-art deep learning models without training from scratch or for those who wish to refine existing models with their own biosignal data.</p>"},{"location":"model_hub/#1-production-models","title":"1. Production Models","text":"<p>Production models are pre-trained neural network models for biosignal processing, available through Hugging Face. These models allow users to perform inference without training and apply state-of-the-art biosignal analysis with minimal setup.</p>"},{"location":"model_hub/#loading-a-pre-trained-model","title":"Loading a Pre-Trained Model","text":"<p>Users can load a model from the NeuralLib Hugging Face collection or any other repository.</p>"},{"location":"model_hub/#example-usage","title":"Example Usage","text":"<pre><code>from NeuralLib.model_hub import ProductionModel\n\n# Load a production model from Hugging Face\nmodel = ProductionModel(model_name=\"my_pretrained_model\")\n\n# Perform inference on a biosignal\npredictions = model.predict(input_signal)\n</code></pre>"},{"location":"model_hub/#available-models","title":"Available Models","text":"<p>To list all pre-trained models available in the NeuralLib Hugging Face collection, use:</p> <pre><code>from NeuralLib.model_hub import list_production_models\n\nlist_production_models()\n</code></pre> <p>This will display all available models that users can download and apply.</p>"},{"location":"model_hub/#2-transfer-learning","title":"2. Transfer Learning","text":"<p>The transfer learning (TL) module provides a flexible way to adapt pre-trained models to new tasks or datasets. Users can:</p> <ul> <li>Load and reuse weights from an existing production model.</li> <li>Inject selected layers into a new architecture.</li> <li>Freeze layers to retain previously learned features.</li> <li>Unfreeze layers to allow further fine-tuning.</li> </ul>"},{"location":"model_hub/#creating-a-transfer-learning-model","title":"Creating a Transfer Learning Model","text":"<p>A TLModel is a modified neural network that uses pre-trained components.</p>"},{"location":"model_hub/#example-usage_1","title":"Example Usage","text":"<pre><code>from NeuralLib.model_hub import TLModel\n\n# Initialize a Transfer Learning Model based on a pre-trained architecture\ntl_model = TLModel(architecture_name=\"GRUseq2seq\", hid_dim=128, n_layers=2, dropout=0.2, learning_rate=0.001)\n</code></pre>"},{"location":"model_hub/#3-fine-tuning-a-pre-trained-model","title":"3. Fine-Tuning a Pre-Trained Model","text":"<p>Once the transfer learning model is created, users can inject weights, freeze/unfreeze layers, and train the model with new data.</p>"},{"location":"model_hub/#step-1-load-a-pre-trained-model","title":"Step 1: Load a Pre-Trained Model","text":"<pre><code>from NeuralLib.model_hub import TLFactory\n\nfactory = TLFactory()\nfactory.load_production_model(\"my_pretrained_model\")\n</code></pre>"},{"location":"model_hub/#step-2-inject-weights-into-a-new-model","title":"Step 2: Inject Weights into a New Model","text":"<pre><code>layer_mapping = {\n    \"model.gru_layers.0\": factory.models[\"my_pretrained_model\"].model.gru_layers[0].state_dict()\n}\n\nfactory.configure_tl_model(tl_model, layer_mapping)\n</code></pre>"},{"location":"model_hub/#step-3-freeze-layers-optional","title":"Step 3: Freeze Layers (Optional)","text":"<pre><code>factory.configure_tl_model(tl_model, layer_mapping, freeze_layers=[\"model.gru_layers.0\"])\n</code></pre>"},{"location":"model_hub/#step-4-train-the-transfer-learning-model","title":"Step 4: Train the Transfer Learning Model","text":"<pre><code>tl_model.train_tl(path_x=\"data/train_x.npy\", path_y=\"data/train_y.npy\", batch_size=32, epochs=10)\n</code></pre>"},{"location":"model_hub/#4-hugging-face-integration","title":"4. Hugging Face Integration","text":"<p>NeuralLib allows users to upload fine-tuned models to Hugging Face, making them available for public or private use.</p>"},{"location":"model_hub/#upload-a-model-to-hugging-face","title":"Upload a Model to Hugging Face","text":"<pre><code>from NeuralLib.model_hub import upload_production_model\n\nupload_production_model(local_dir=\"my_model_dir\", repo_name=\"my-huggingface-repo\", token=\"YOUR_HF_TOKEN\", model_name=\"MyFineTunedModel\")\n</code></pre> <p>This function automatically generates a <code>README.md</code> file, organizes the model files, and pushes them to Hugging Face.</p>"},{"location":"model_hub/#hands-on","title":"Hands on","text":"<p>Check Tutorials:</p> <ol> <li> <p>Tutorial #4: Use a production model.</p> <p>Key methods: <code>list_production_models()</code>, <code>ProductionModel</code> and <code>predict</code></p> </li> <li> <p>Tutorial #5: Perform transfer-learning: load a production model to your TLFactory, create a TLModel and train it leveraging the weights of the chosen pre-trained model.</p> <p>Key methods: <code>load_production_model()</code>, <code>TLModel</code>, <code>configure_tl_model</code>, <code>train_tl</code></p> </li> </ol>"},{"location":"model_hub/#summary","title":"Summary","text":"<p>The <code>model_hub</code> module provides tools for loading, modifying, and fine-tuning pre-trained biosignal models. It allows users to apply, refine, and share deep learning models with minimal effort.</p>"},{"location":"training_deployment/","title":"Training and Deployment Workflow in NeuralLib","text":"<p>This section describes how to train, fine-tune, post-process, and deploy models using NeuralLib.</p>"},{"location":"training_deployment/#1-training-and-retraining-models","title":"1. Training and Retraining Models","text":""},{"location":"training_deployment/#available-architectures","title":"Available Architectures","text":"<p>To see a list of available architectures, use:</p> <pre><code>from NeuralLib.architectures import get_valid_architectures\nprint(get_valid_architectures())\n</code></pre> <p>Make sure to validate architecture names before training:</p> <pre><code>from NeuralLib.architectures import validate_architecture_name\nvalidate_architecture_name(\"GRUseq2seq\")\n</code></pre>"},{"location":"training_deployment/#training-a-model-from-scratch","title":"Training a Model from Scratch","text":"<p>To train a model from scratch, instantiate it and call the <code>train_from_scratch</code> method:</p> <pre><code>from NeuralLib.architectures import GRUseq2seq\n\nmodel = GRUseq2seq(model_name=\"test\", n_features=10, hid_dim=64, n_layers=2, dropout=0.1, learning_rate=0.001)\n\nmodel.train_from_scratch(\n    path_x=\"data/train_x.npy\",\n    path_y=\"data/train_y.npy\",\n    batch_size=32,\n    epochs=50\n)\n</code></pre> <p>Alternatively, you can use a higher-level function to automate training:</p> <pre><code>python\nCopiarEditar\nfrom NeuralLib.training import train_architecture_from_scratch\n\ntrain_architecture_from_scratch(\n    architecture_name=\"GRUseq2seq\",\n    architecture_params={\"n_features\": 10, \"hid_dim\": 64, \"n_layers\": 2, \"dropout\": 0.1, \"learning_rate\": 0.001},\n    train_params={\"path_x\": \"data/train_x.npy\", \"path_y\": \"data/train_y.npy\", \"batch_size\": 32, \"epochs\": 50}\n)\n</code></pre>"},{"location":"training_deployment/#retraining-an-existing-model","title":"Retraining an Existing Model","text":"<p>If you have a checkpoint from a previous training session, you can continue training:</p> <pre><code>python\nCopiarEditar\nfrom NeuralLib.training import retrain_architecture\n\nretrain_architecture(\n    architecture_name=\"GRUseq2seq\",\n    train_params={\"batch_size\": 32, \"epochs\": 20},\n    checkpoints_directory=\"checkpoints/GRUseq2seq_run1\"\n)\n</code></pre> <p>You can also load and retrain models from Hugging Face:</p> <pre><code>python\nCopiarEditar\nretrain_architecture(\n    architecture_name=\"GRUseq2seq\",\n    train_params={\"batch_size\": 32, \"epochs\": 20},\n    hugging_face_model=\"marianaagdias/GRU_ecg_model\"\n)\n</code></pre>"},{"location":"training_deployment/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Grid search can be used to find the best hyperparameters automatically:</p> <pre><code>python\nCopiarEditar\nfrom NeuralLib.training import run_grid_search\n\nbest_model = run_grid_search(\n    architecture_name=\"GRUseq2seq\",\n    architecture_params_options={\n        \"hid_dim\": [32, 64, 128],\n        \"dropout\": [0.1, 0.2, 0.3],\n        \"learning_rate\": [0.001, 0.0005]\n    },\n    train_params={\"batch_size\": 32, \"epochs\": 50}\n)\n</code></pre> <p>\ud83d\udca1 This function iterates over all parameter combinations and selects the best performing model.</p>"},{"location":"training_deployment/#2-post-processing-model-outputs","title":"2. Post-Processing Model Outputs","text":"<p>Once a model has been trained, you may need to post-process its output. This is particularly useful for peak detection or classification thresholds.</p>"},{"location":"training_deployment/#binary-peak-detection","title":"Binary Peak Detection","text":"<pre><code>python\nCopiarEditar\nfrom NeuralLib.processing import post_process_peaks_binary\n\npredictions = model.predict(input_signal)\n\nfiltered_peaks = post_process_peaks_binary(\n    predictions,\n    threshold=0.5,\n    filter_peaks=True\n)\n</code></pre> <p>This function: \u2714 Applies a sigmoid activation</p> <p>\u2714 Thresholds the output to detect peaks</p> <p>\u2714 (Optional) Filters out closely spaced peaks</p>"},{"location":"training_deployment/#3-deploying-models-to-hugging-face","title":"3. Deploying Models to Hugging Face","text":"<p>Once a model is trained, you can upload it to Hugging Face to make it publicly available.</p>"},{"location":"training_deployment/#generating-a-model-card","title":"Generating a Model Card","text":"<p>To ensure proper documentation, you must create a structured <code>README.md</code>:</p> <pre><code>python\nCopiarEditar\nfrom NeuralLib.upload import create_readme\n\ncreate_readme(\n    hparams_file=\"model/hparams.yaml\",\n    training_info_file=\"model/training_info.json\",\n    output_file=\"model/README.md\",\n    collection=\"NeuralLib Collection\",\n    description=\"A deep learning model for ECG signal classification.\",\n    model_name=\"GRUseq2seq\"\n)\n</code></pre> <p>This automatically formats:</p> <ul> <li>Hyperparameters</li> <li>Training results</li> <li>Example usage</li> </ul>"},{"location":"training_deployment/#uploading-the-model","title":"Uploading the Model","text":"<p>To push the model to Hugging Face Model Hub:</p> <pre><code>python\nCopiarEditar\nfrom NeuralLib.upload import upload_production_model\n\nupload_production_model(\n    local_dir=\"models/my_model\",\n    repo_name=\"marianaagdias/my_ecg_model\",\n    token=\"your-huggingface-token\",\n    model_name=\"GRUseq2seq\",\n    description=\"This model detects arrhythmias in ECG signals.\"\n)\n</code></pre> <p>\u2714 If the repository doesn\u2019t exist, it will be created automatically.</p> <p>\u2714 The model is now publicly available and can be used by others.</p>"},{"location":"training_deployment/#final-notes","title":"Final Notes","text":"<ul> <li>Training (<code>train_architecture_from_scratch</code>, <code>retrain_architecture</code>, <code>run_grid_search</code>)</li> <li>Post-Processing (<code>post_process_peaks_binary</code>)</li> <li>Deployment (<code>create_readme</code>, <code>upload_production_model</code>)</li> </ul> <p>This section ensures that developers and researchers can: \u2714 Train and fine-tune models</p> <p>\u2714 Apply post-processing</p> <p>\u2714 Deploy models to Hugging Face</p>"},{"location":"utils/","title":"utils","text":""},{"location":"utils/#overview","title":"Overview","text":"<p>The <code>utils</code> module provides essential utility functions and a dataset class (<code>DatasetSequence</code>) to support training and inference workflows in <code>NeuralLib</code>. It includes:</p> <ul> <li>Dataset Management: <code>DatasetSequence</code> for handling biosignal datasets.</li> <li>Device Configuration: Functions to set up GPUs and reproducibility.</li> <li>Model Handling: Functions for saving models and predictions.</li> </ul> <p>Check Dataset Requirements for using DatasetSequence class to train your models.</p>"},{"location":"utils/#1-dataset-management","title":"1. Dataset Management","text":""},{"location":"utils/#datasetsequence-class","title":"<code>DatasetSequence</code> Class","text":"<p>The <code>DatasetSequence</code> class is a PyTorch Dataset wrapper that loads preprocessed biosignal data stored as <code>.npy</code> files. It ensures proper formatting, normalization, and data consistency for training and inference.</p>"},{"location":"utils/#initialization-parameters","title":"Initialization Parameters","text":"<pre><code>DatasetSequence(path_x, path_y, part='train', all_samples=False, samples=None, seq2one=False, min_max_norm_sig=False, window_size=None, overlap=None)\n</code></pre> <ul> <li><code>path_x</code> (str) \u2192 Path to the directory containing input <code>.npy</code> files.</li> <li><code>path_y</code> (str) \u2192 Path to the directory containing output <code>.npy</code> files.</li> <li><code>part</code> (str, default='train') \u2192 Dataset partition to use (<code>'train'</code>, <code>'val'</code>, or <code>'test'</code>).</li> <li><code>all_samples</code> (bool, default=False) \u2192 If <code>True</code>, loads all available samples; otherwise, a subset is used.</li> <li><code>samples</code> (int, optional) \u2192 Number of samples to load (must be set if <code>all_samples=False</code>).</li> <li><code>seq2one</code> (bool, default=False) \u2192 Defines whether the dataset is sequence-to-one (<code>True</code>) or sequence-to-sequence (<code>False</code>).</li> <li><code>min_max_norm_sig</code> (bool, default=False) \u2192 Whether to apply Min-Max Normalization to each signal.</li> <li><code>window_size</code> (int, optional) \u2192 Not yet implemented (for potential sliding window functionality).</li> <li><code>overlap</code> (float, optional) \u2192 Not yet implemented (for potential window overlap functionality).</li> </ul>"},{"location":"utils/#key-features","title":"Key Features","text":"<p>\u2714\ufe0f Loads <code>.npy</code> files efficiently.</p> <p>\u2714\ufe0f Auto-adjusts input shape (<code>seq_len, num_features</code>).</p> <p>\u2714\ufe0f Ensures output consistency for <code>seq2seq</code> and <code>seq2one</code> tasks.</p> <p>\u2714\ufe0f Optional min-max normalization of input/output.</p>"},{"location":"utils/#2-device-and-reproducibility-functions","title":"2. Device and Reproducibility Functions","text":""},{"location":"utils/#configure_seedseed","title":"<code>configure_seed(seed)</code>","text":"<pre><code>configure_seed(42)\n</code></pre> <p>Sets the random seed for NumPy, PyTorch, and CUDA to ensure reproducibility.</p>"},{"location":"utils/#configure_devicegpu_idnone","title":"<code>configure_device(gpu_id=None)</code>","text":"<pre><code>device = configure_device()\n</code></pre> <p>Automatically configures GPU (if available) or CPU, printing the selected device.</p>"},{"location":"utils/#list_gpus","title":"<code>list_gpus()</code>","text":"<p>Lists all available GPUs.</p>"},{"location":"utils/#3-model-and-prediction-utilities","title":"3. Model and Prediction Utilities","text":""},{"location":"utils/#save_model_resultsmodel-results_dir-model_name-best_val_loss","title":"<code>save_model_results(model, results_dir, model_name, best_val_loss)</code>","text":"<pre><code>save_model_results(model, \"results\", \"GRU_model\", 0.25)\n</code></pre> <p>Saves the model hyperparameters and best validation loss in a <code>results.json</code> file.</p>"},{"location":"utils/#save_predictionspredictions-batch_idx-dir","title":"<code>save_predictions(predictions, batch_idx, dir)</code>","text":"<pre><code>save_predictions(predictions, 3, \"predictions_dir\")\n</code></pre> <p>Saves predictions as <code>.npy</code> files.</p>"},{"location":"utils/#save_predictions_with_filenamepredictions-input_filename-dir","title":"<code>save_predictions_with_filename(predictions, input_filename, dir)</code>","text":"<pre><code>save_predictions_with_filename(predictions, \"sample_101.npy\", \"output_dir\")\n</code></pre> <p>Saves predictions using the input filename for easier tracking.</p>"},{"location":"utils/#4-data-formatting-and-collation","title":"4. Data Formatting and Collation","text":""},{"location":"utils/#collate_fnbatch","title":"<code>collate_fn(batch)</code>","text":"<pre><code>train_dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n</code></pre> <p>Custom collation function for variable-length sequences. It: \u2714 Sorts sequences by length for efficient batch processing.</p> <p>\u2714 Pads sequences to the longest sequence in the batch.</p>"},{"location":"utils/#5-lossplotcallback","title":"5. LossPlotCallback","text":"<p><code>LossPlotCallback</code> is a custom PyTorch Lightning callback designed to track and visualize training and validation loss over epochs. At the end of training, it generates a loss plot and saves it to a specified path.</p> <pre><code>from NeuralLib.utils import LossPlotCallback\ntrainer = pl.Trainer(callbacks=[LossPlotCallback(\"results/loss_plot.png\")])\n</code></pre>"},{"location":"utils/#key-features_1","title":"Key Features","text":"<ul> <li>Automatically tracks <code>train_loss</code> and <code>val_loss</code> at the end of each epoch.</li> <li>Saves a loss plot (<code>.png</code>) to the specified path at the end of training.</li> <li>Compatible with PyTorch Lightning's callback system.</li> </ul>"},{"location":"utils/#methods","title":"Methods","text":"<ul> <li><code>on_train_epoch_end(trainer, pl_module)</code>: Logs training loss at the end of each epoch.</li> <li><code>on_validation_epoch_end(trainer, pl_module)</code>: Logs validation loss at the end of each epoch.</li> <li><code>on_train_end(trainer, pl_module)</code>: Generates and saves the loss plot.</li> </ul> <p>Dataset Organization and Requirements</p>"}]}